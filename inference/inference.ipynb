{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3c269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python314\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data & Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/615 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 1. 설정 (Train과 동일하게) ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"bert-base-uncased\" # 학습할 때 쓴 모델명과 동일해야 함\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 531\n",
    "INPUT_MODEL_PATH = \"saved_model/best_model.pt\"\n",
    "OUTPUT_CSV = \"submission.csv\"\n",
    "\n",
    "# 경로 (사용자 환경에 맞게 수정)\n",
    "BASE_DIR = \"Amazon_products\"\n",
    "TEST_CORPUS_PATH = os.path.join(BASE_DIR, \"test/test_corpus.txt\") # 파일명 확인\n",
    "HIERARCHY_PATH = os.path.join(BASE_DIR, \"class_hierarchy.txt\")\n",
    "\n",
    "# --- 2. 헬퍼 함수들 ---\n",
    "def load_test_corpus(path):\n",
    "    pids, texts = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pids.append(parts[0])\n",
    "                texts.append(parts[1])\n",
    "    return pids, texts\n",
    "\n",
    "def load_hierarchy(path):\n",
    "    parents = defaultdict(list)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                p, c = int(parts[0]), int(parts[1])\n",
    "                parents[c].append(p)\n",
    "    return parents\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "# --- 3. 추론 실행 ---\n",
    "def run_inference():\n",
    "    print(\"Loading Data & Model...\")\n",
    "    test_pids, test_texts = load_test_corpus(TEST_CORPUS_PATH)\n",
    "    parents_map = load_hierarchy(HIERARCHY_PATH)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    dataset = TestDataset(test_texts, tokenizer, MAX_LEN)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    # 모델 로드\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_CLASSES)\n",
    "    model.load_state_dict(torch.load(INPUT_MODEL_PATH, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    \n",
    "    print(\"Predicting...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # 확률로 변환 (Sigmoid)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            \n",
    "            # 상위 K개 추출 (여기서는 Threshold 대신 Top-K 전략 사용 + 계층구조)\n",
    "            # 과제에서 라벨이 2~3개라고 했으므로, 넉넉히 Top-3를 뽑고 부모를 추가하는 전략\n",
    "            top_k = 3\n",
    "            top_probs, top_indices = torch.topk(probs, k=top_k, dim=1)\n",
    "            \n",
    "            top_indices = top_indices.cpu().numpy()\n",
    "            \n",
    "            for idx_list in top_indices:\n",
    "                final_labels = set(idx_list)\n",
    "                \n",
    "                # 계층 구조 보정 (자식이 있으면 부모도 반드시 포함)\n",
    "                # 2~3번 반복해서 최상위 부모까지 찾아감\n",
    "                for _ in range(3): \n",
    "                    current_ids = list(final_labels)\n",
    "                    for cid in current_ids:\n",
    "                        if cid in parents_map:\n",
    "                            for pid in parents_map[cid]:\n",
    "                                final_labels.add(pid)\n",
    "                \n",
    "                all_preds.append(sorted(list(final_labels)))\n",
    "\n",
    "    # --- 4. CSV 저장 ---\n",
    "    print(f\"Saving submission to {OUTPUT_CSV}...\")\n",
    "    with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\", \"labels\"])\n",
    "        for pid, labels in zip(test_pids, all_preds):\n",
    "            label_str = \",\".join(map(str, labels))\n",
    "            writer.writerow([pid, label_str])\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
