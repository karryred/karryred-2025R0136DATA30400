{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb563ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 설정 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 531\n",
    "\n",
    "PATH_BERT_MODEL = \"saved_ensemble/bert_final.pt\"\n",
    "PATH_DEBERTA_MODEL = \"saved_ensemble/deberta_final.pt\"\n",
    "OUTPUT_CSV = \"submission.csv\"\n",
    "\n",
    "BASE_DIR = \"Amazon_products\"\n",
    "TEST_CORPUS_PATH = os.path.join(BASE_DIR, \"test/test_corpus.txt\")\n",
    "HIERARCHY_PATH = os.path.join(BASE_DIR, \"class_hierarchy.txt\")\n",
    "\n",
    "# --- [핵심 수정] 통합 모델 클래스 정의 ---\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias: self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else: self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None: self.bias.data.uniform_(-stdv, stdv)\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output + self.bias if self.bias is not None else output\n",
    "\n",
    "class GNN_Model(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, hidden_dim, adj_matrix, model_type=\"bert\"):\n",
    "        super(GNN_Model, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.label_embedding = nn.Parameter(torch.FloatTensor(num_classes, hidden_dim))\n",
    "        nn.init.xavier_uniform_(self.label_embedding)\n",
    "        self.gcn = GraphConvolution(hidden_dim, hidden_dim)\n",
    "        self.adj_matrix = adj_matrix\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        if self.model_type == \"bert\":\n",
    "            doc_embedding = outputs.pooler_output\n",
    "        else: # deberta\n",
    "            doc_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "            \n",
    "        doc_embedding = self.dropout(doc_embedding)\n",
    "        refined_label = torch.tanh(self.gcn(self.label_embedding, self.adj_matrix))\n",
    "        return torch.mm(doc_embedding, refined_label.t())\n",
    "\n",
    "# --- 데이터 로더 ---\n",
    "def load_test_data():\n",
    "    pids, texts = [], []\n",
    "    with open(TEST_CORPUS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pids.append(parts[0])\n",
    "                texts.append(parts[1])\n",
    "    return pids, texts\n",
    "\n",
    "def load_hierarchy():\n",
    "    parents = defaultdict(list)\n",
    "    if os.path.exists(HIERARCHY_PATH):\n",
    "        with open(HIERARCHY_PATH, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    p, c = int(parts[0]), int(parts[1])\n",
    "                    if p < NUM_CLASSES and c < NUM_CLASSES: parents[c].append(p)\n",
    "    return parents\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, item):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            str(self.texts[item]), add_special_tokens=True, max_length=self.max_len,\n",
    "            return_token_type_ids=False, padding='max_length', truncation=True,\n",
    "            return_attention_mask=True, return_tensors='pt'\n",
    "        )\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten()}\n",
    "\n",
    "# --- 실행 ---\n",
    "def run_ensemble():\n",
    "    print(\"1. Loading Data...\")\n",
    "    test_pids, test_texts = load_test_data()\n",
    "    parents_map = load_hierarchy()\n",
    "    \n",
    "    # Tokenizer 로드 (use_fast=False 필수)\n",
    "    print(\"   - Loading Tokenizers...\")\n",
    "    tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer_deberta = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\", use_fast=False)\n",
    "    \n",
    "    # Dataset & Loader 생성\n",
    "    loader_bert = DataLoader(TestDataset(test_texts, tokenizer_bert, MAX_LEN), batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    loader_deberta = DataLoader(TestDataset(test_texts, tokenizer_deberta, MAX_LEN), batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    print(\"2. Loading Models...\")\n",
    "    # 통합 클래스(GNN_Model)로 로드 (weights_only=False 필수)\n",
    "    model_bert = torch.load(PATH_BERT_MODEL, map_location=DEVICE, weights_only=False)\n",
    "    model_bert.to(DEVICE).eval()\n",
    "    \n",
    "    model_deberta = torch.load(PATH_DEBERTA_MODEL, map_location=DEVICE, weights_only=False)\n",
    "    model_deberta.to(DEVICE).eval()\n",
    "\n",
    "    print(\"3. Ensembling...\")\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_bert, batch_deberta in tqdm(zip(loader_bert, loader_deberta), total=len(loader_bert)):\n",
    "            \n",
    "            # BERT Forward\n",
    "            ids_b = batch_bert['input_ids'].to(DEVICE)\n",
    "            mask_b = batch_bert['attention_mask'].to(DEVICE)\n",
    "            logits_bert = model_bert(ids_b, mask_b)\n",
    "            \n",
    "            # DeBERTa Forward\n",
    "            ids_d = batch_deberta['input_ids'].to(DEVICE)\n",
    "            mask_d = batch_deberta['attention_mask'].to(DEVICE)\n",
    "            logits_deberta = model_deberta(ids_d, mask_d)\n",
    "\n",
    "            # Soft Voting (DeBERTa 가중치 0.6)\n",
    "            probs = (torch.sigmoid(logits_bert) * 0.4) + (torch.sigmoid(logits_deberta) * 0.6)\n",
    "            batch_probs = probs.cpu().numpy()\n",
    "            \n",
    "            # 후처리 (계층 구조 보정 & 2~3개 선택)\n",
    "            for sample_probs in batch_probs:\n",
    "                top_indices = sample_probs.argsort()[-10:][::-1]\n",
    "                candidate_set = set()\n",
    "                score_map = {}\n",
    "\n",
    "                for cid in top_indices:\n",
    "                    candidate_set.add(cid)\n",
    "                    score_map[cid] = float(sample_probs[cid])\n",
    "                    curr, depth = cid, 0\n",
    "                    while curr in parents_map and depth < 5:\n",
    "                        for pid in parents_map[curr]:\n",
    "                            if pid < NUM_CLASSES:\n",
    "                                candidate_set.add(pid)\n",
    "                                if pid not in score_map: score_map[pid] = float(sample_probs[pid])\n",
    "                            curr = pid\n",
    "                        depth += 1\n",
    "                \n",
    "                sorted_candidates = sorted(list(candidate_set), key=lambda x: score_map.get(x, 0), reverse=True)\n",
    "                final_labels = sorted_candidates[:3]\n",
    "                \n",
    "                if len(final_labels) < 2:\n",
    "                    rem = [x for x in sample_probs.argsort()[::-1] if x not in final_labels]\n",
    "                    for r in rem:\n",
    "                        final_labels.append(r)\n",
    "                        if len(final_labels) >= 2: break\n",
    "                \n",
    "                all_preds.append(sorted(final_labels))\n",
    "\n",
    "    print(f\"4. Saving to {OUTPUT_CSV}...\")\n",
    "    with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\", \"labels\"])\n",
    "        for pid, labels in zip(test_pids, all_preds):\n",
    "            writer.writerow([pid, \",\".join(map(str, labels))])\n",
    "    print(\"✅ Ensemble Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_ensemble()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
