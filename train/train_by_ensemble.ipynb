{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1665cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- ì„¤ì • ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16 \n",
    "EPOCHS = 5\n",
    "NUM_CLASSES = 531\n",
    "HIDDEN_DIM = 768\n",
    "\n",
    "# ê²½ë¡œ\n",
    "BASE_DIR = \"Amazon_products\"\n",
    "TRAIN_CORPUS_PATH = os.path.join(BASE_DIR, \"train/train_corpus.txt\")\n",
    "TEST_CORPUS_PATH = os.path.join(BASE_DIR, \"test/test_corpus.txt\")\n",
    "HIERARCHY_PATH = os.path.join(BASE_DIR, \"class_hierarchy.txt\")\n",
    "OUTPUT_DIR = \"saved_ensemble\"\n",
    "\n",
    "# ì‚¬ìš©í•  ë°ì´í„° íŒŒì¼ë“¤\n",
    "SILVER_FILE = \"train_round_2.csv\"\n",
    "GOLD_FILE_1 = \"gold_labels.csv\"\n",
    "GOLD_FILE_2 = \"gold_labels2.csv\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# --- ë°ì´í„° ë¡œë“œ ë° ë³‘í•© ---\n",
    "def load_merged_data():\n",
    "    print(\"1. Merging Data & Cleaning...\")\n",
    "    id2text = {}\n",
    "    \n",
    "    # ID ì •ì œ í•¨ìˆ˜\n",
    "    def clean_id(x):\n",
    "        try: return str(int(float(x)))\n",
    "        except: return str(x).strip()\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ë¡œë“œ\n",
    "    loaded_files = 0\n",
    "    for path in [TRAIN_CORPUS_PATH, TEST_CORPUS_PATH]:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(\"\\t\", 1)\n",
    "                    if len(parts) == 2:\n",
    "                        idx = clean_id(parts[0])\n",
    "                        id2text[idx] = parts[1]\n",
    "            loaded_files += 1\n",
    "    \n",
    "    if loaded_files == 0:\n",
    "        raise FileNotFoundError(\"âŒ Error: corpus files not found.\")\n",
    "\n",
    "    # CSV ë¡œë“œ\n",
    "    dfs = []\n",
    "    input_files = [SILVER_FILE, GOLD_FILE_1, GOLD_FILE_2]\n",
    "    \n",
    "    for filepath in input_files:\n",
    "        if os.path.exists(filepath):\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                # ì»¬ëŸ¼ëª… í‘œì¤€í™”\n",
    "                df.columns = [c.strip().lower() for c in df.columns]\n",
    "                if 'pid' in df.columns: df.rename(columns={'pid': 'id'}, inplace=True)\n",
    "                \n",
    "                if 'id' not in df.columns or 'labels' not in df.columns:\n",
    "                    print(f\"   âš ï¸ Skipping {filepath}: columns missing\")\n",
    "                    continue\n",
    "\n",
    "                df['id'] = df['id'].apply(clean_id)\n",
    "                \n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ Error reading {filepath}: {e}\")\n",
    "        else:\n",
    "            if filepath == SILVER_FILE:\n",
    "                raise FileNotFoundError(f\"âŒ Error: {SILVER_FILE} not found.\")\n",
    "\n",
    "    df_final = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # [í•µì‹¬] ë¼ë²¨ ë°ì´í„° í´ë¦¬ë‹ (None, NaN, ë¬¸ìì—´ ë“± ì œê±°)\n",
    "    def is_valid_label(x):\n",
    "        s = str(x).strip().lower()\n",
    "        if s in ['none', 'nan', '', 'null']: return False\n",
    "        # ìˆ«ìê°€ í•˜ë‚˜ë¼ë„ ìˆì–´ì•¼ í•¨\n",
    "        return any(c.isdigit() for c in s)\n",
    "\n",
    "    # ë¶ˆëŸ‰ ë¼ë²¨ ì œê±°\n",
    "    initial_len = len(df_final)\n",
    "    df_final = df_final[df_final['labels'].apply(is_valid_label)]\n",
    "    cleaned_len = len(df_final)\n",
    "    if initial_len != cleaned_len:\n",
    "        print(f\"   ğŸ§¹ Cleaned {initial_len - cleaned_len} invalid label rows.\")\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ë§¤í•‘ & ê²°ì¸¡ì¹˜ ì œê±°\n",
    "    df_final['text'] = df_final['id'].map(id2text)\n",
    "    df_final = df_final.dropna(subset=['text', 'labels'])\n",
    "    \n",
    "    print(f\"   âœ… Final Training Size: {len(df_final)}\")\n",
    "    return df_final\n",
    "\n",
    "# --- í†µí•© ëª¨ë¸ í´ë˜ìŠ¤ ---\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias: self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else: self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None: self.bias.data.uniform_(-stdv, stdv)\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output + self.bias if self.bias is not None else output\n",
    "\n",
    "class GNN_Model(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, hidden_dim, adj_matrix, model_type=\"bert\"):\n",
    "        super(GNN_Model, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.label_embedding = nn.Parameter(torch.FloatTensor(num_classes, hidden_dim))\n",
    "        nn.init.xavier_uniform_(self.label_embedding)\n",
    "        self.gcn = GraphConvolution(hidden_dim, hidden_dim)\n",
    "        self.adj_matrix = adj_matrix\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì„ë² ë”© ì¶”ì¶œ ë°©ì‹ ë¶„ê¸° ì²˜ë¦¬\n",
    "        if self.model_type == \"bert\":\n",
    "            doc_embedding = outputs.pooler_output\n",
    "        else: # deberta\n",
    "            doc_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "            \n",
    "        doc_embedding = self.dropout(doc_embedding)\n",
    "        refined_label = torch.tanh(self.gcn(self.label_embedding, self.adj_matrix))\n",
    "        return torch.mm(doc_embedding, refined_label.t())\n",
    "\n",
    "# --- í•™ìŠµ í•¨ìˆ˜ ---\n",
    "def train_one_model(model_name, save_name, df, adj_matrix, model_type):\n",
    "    print(f\"\\nğŸš€ Training Model: {model_name} ({model_type})\")\n",
    "    \n",
    "    # Tokenizer (use_fast=False í•„ìˆ˜)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    \n",
    "    class ReviewDataset(Dataset):\n",
    "        def __init__(self, texts, labels):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "        def __len__(self): return len(self.texts)\n",
    "        def __getitem__(self, item):\n",
    "            lbls = [int(x) for x in str(self.labels[item]).split(\",\") if x]\n",
    "            lbl_tensor = torch.zeros(NUM_CLASSES)\n",
    "            lbl_tensor[lbls] = 1.0\n",
    "            enc = tokenizer.encode_plus(str(self.texts[item]), padding='max_length', \n",
    "                                      truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "            return {'ids': enc['input_ids'].flatten(), 'mask': enc['attention_mask'].flatten(), 'lbl': lbl_tensor}\n",
    "\n",
    "    # Dataset & Loader\n",
    "    dataset = ReviewDataset(df.text.to_numpy(), df.labels.to_numpy())\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Model Init\n",
    "    model = GNN_Model(model_name, NUM_CLASSES, HIDDEN_DIM, adj_matrix, model_type).to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5) # í•™ìŠµë¥  í†µì¼\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, 0, len(loader) * EPOCHS)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            ids, mask, lbl = batch['ids'].to(DEVICE), batch['mask'].to(DEVICE), batch['lbl'].to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(ids, mask)\n",
    "            loss = loss_fn(logits, lbl)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"   Loss: {total_loss / len(loader):.4f}\")\n",
    "    \n",
    "    # Save\n",
    "    save_path = os.path.join(OUTPUT_DIR, save_name)\n",
    "    torch.save(model, save_path)\n",
    "    print(f\"âœ… Model Saved: {save_path}\")\n",
    "\n",
    "# --- ë©”ì¸ ì‹¤í–‰ ---\n",
    "def main():\n",
    "    # 1. ë°ì´í„° ì¤€ë¹„\n",
    "    df = load_merged_data()\n",
    "    \n",
    "    # 2. Adjacency Matrix ìƒì„±\n",
    "    adj = torch.eye(NUM_CLASSES)\n",
    "    with open(HIERARCHY_PATH, \"r\") as f:\n",
    "        for line in f:\n",
    "            p, c = map(int, line.split())\n",
    "            if p < NUM_CLASSES and c < NUM_CLASSES: adj[p,c]=1; adj[c,p]=1\n",
    "    rowsum = torch.sum(adj, dim=1)\n",
    "    d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "    adj = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt).to(DEVICE)\n",
    "\n",
    "    # 3. ìˆœì°¨ í•™ìŠµ ì§„í–‰\n",
    "    # (1) BERT ëª¨ë¸ í•™ìŠµ\n",
    "    train_one_model(\"bert-base-uncased\", \"bert_final.pt\", df, adj, \"bert\")\n",
    "    \n",
    "    # (2) DeBERTa ëª¨ë¸ í•™ìŠµ\n",
    "    train_one_model(\"microsoft/deberta-v3-base\", \"deberta_final.pt\", df, adj, \"deberta\")\n",
    "\n",
    "    print(\"\\nğŸ‰ All Training Completed! Ready for Ensemble Inference.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
