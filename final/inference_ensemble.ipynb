{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64beb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Data...\n",
      "   - Loading BERT Tokenizer...\n",
      "   - Loading DeBERTa Tokenizer...\n",
      "2. Loading Models...\n",
      "3. Ensembling...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from collections import defaultdict\n",
    "import transformers.models.deberta_v2.modeling_deberta_v2 as deberta_v2_mod\n",
    "\n",
    "# StableDropout이 모듈에 없으면 강제로 주입\n",
    "if not hasattr(deberta_v2_mod, 'StableDropout'):\n",
    "    try:\n",
    "        # 최신 버전에는 DebertaV2StableDropout이라는 이름으로 존재할 가능성이 높음\n",
    "        deberta_v2_mod.StableDropout = deberta_v2_mod.DebertaV2StableDropout\n",
    "    except AttributeError:\n",
    "        # 만약 그것도 없다면 일반 Dropout으로 대체 (일반적인 추론에는 문제 없음)\n",
    "        deberta_v2_mod.StableDropout = torch.nn.Dropout\n",
    "\n",
    "# --- 설정 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 531\n",
    "\n",
    "# 경로 확인\n",
    "PATH_BERT_MODEL = \"saved_model_gnn/best_model_gnn.pt\"         \n",
    "PATH_DEBERTA_MODEL = \"saved_model_deberta/best_model_deberta.pt\" \n",
    "\n",
    "BASE_DIR = \"../Amazon_products\"\n",
    "TEST_CORPUS_PATH = os.path.join(BASE_DIR, \"test/test_corpus.txt\")\n",
    "HIERARCHY_PATH = os.path.join(BASE_DIR, \"class_hierarchy.txt\")\n",
    "OUTPUT_CSV = \"final_submission.csv\"\n",
    "\n",
    "# --- 모델 클래스 정의 ---\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output + self.bias if self.bias is not None else output\n",
    "\n",
    "\n",
    "class BertGCN(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, hidden_dim, adj_matrix):\n",
    "        super(BertGCN, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.label_embedding = nn.Parameter(torch.FloatTensor(num_classes, hidden_dim))\n",
    "        self.gcn = GraphConvolution(hidden_dim, hidden_dim)\n",
    "        self.adj_matrix = adj_matrix\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        doc_embedding = self.dropout(outputs.pooler_output)\n",
    "        refined_label_embedding = torch.tanh(self.gcn(self.label_embedding, self.adj_matrix))\n",
    "        return torch.mm(doc_embedding, refined_label_embedding.t())\n",
    "\n",
    "\n",
    "class DebertaGCN(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, hidden_dim, adj_matrix):\n",
    "        super(DebertaGCN, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.label_embedding = nn.Parameter(torch.FloatTensor(num_classes, hidden_dim))\n",
    "        self.gcn = GraphConvolution(hidden_dim, hidden_dim)\n",
    "        self.adj_matrix = adj_matrix\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        doc_embedding = self.dropout(outputs.last_hidden_state[:, 0, :]) \n",
    "        refined_label_embedding = torch.tanh(self.gcn(self.label_embedding, self.adj_matrix))\n",
    "        return torch.mm(doc_embedding, refined_label_embedding.t())\n",
    "\n",
    "\n",
    "# --- 데이터셋 ---\n",
    "def load_test_data():\n",
    "    pids, texts = [], []\n",
    "    with open(TEST_CORPUS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pids.append(parts[0])\n",
    "                texts.append(parts[1])\n",
    "    return pids, texts\n",
    "\n",
    "\n",
    "def load_hierarchy():\n",
    "    parents = defaultdict(list)\n",
    "    if os.path.exists(HIERARCHY_PATH):\n",
    "        with open(HIERARCHY_PATH, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    p, c = int(parts[0]), int(parts[1])\n",
    "                    if p < NUM_CLASSES and c < NUM_CLASSES:\n",
    "                        parents[c].append(p)\n",
    "    return parents\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            str(self.texts[item]),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "\n",
    "# --- 실행 ---\n",
    "def run_ensemble():\n",
    "    print(\"1. Loading Data...\")\n",
    "    test_pids, test_texts = load_test_data()\n",
    "    parents_map = load_hierarchy()\n",
    "    \n",
    "    # 토크나이저를 각각 따로 로드\n",
    "    print(\"   - Loading BERT Tokenizer...\")\n",
    "    tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    print(\"   - Loading DeBERTa Tokenizer...\")\n",
    "    tokenizer_deberta = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\", use_fast=False)\n",
    "    \n",
    "    # 데이터셋과 로더도 각각 따로 생성\n",
    "    dataset_bert = TestDataset(test_texts, tokenizer_bert, MAX_LEN)\n",
    "    loader_bert = DataLoader(dataset_bert, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    \n",
    "    dataset_deberta = TestDataset(test_texts, tokenizer_deberta, MAX_LEN)\n",
    "    loader_deberta = DataLoader(dataset_deberta, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    print(\"2. Loading Models...\")\n",
    "    model_bert = torch.load(PATH_BERT_MODEL, map_location=DEVICE, weights_only=False)\n",
    "    model_bert.to(DEVICE).eval()\n",
    "    \n",
    "    model_deberta = torch.load(PATH_DEBERTA_MODEL, map_location=DEVICE, weights_only=False)\n",
    "    model_deberta.to(DEVICE).eval()\n",
    "\n",
    "    print(\"3. Ensembling...\")\n",
    "    all_preds = []\n",
    "    \n",
    "    # 두 로더를 zip으로 묶어서 동시에 순회\n",
    "    with torch.no_grad():\n",
    "        for batch_bert, batch_deberta in tqdm(zip(loader_bert, loader_deberta), total=len(loader_bert)):\n",
    "            \n",
    "            # BERT 입력\n",
    "            ids_b = batch_bert['input_ids'].to(DEVICE)\n",
    "            mask_b = batch_bert['attention_mask'].to(DEVICE)\n",
    "            logits_bert = model_bert(ids_b, mask_b)\n",
    "            \n",
    "            # DeBERTa 입력\n",
    "            ids_d = batch_deberta['input_ids'].to(DEVICE)\n",
    "            mask_d = batch_deberta['attention_mask'].to(DEVICE)\n",
    "            logits_deberta = model_deberta(ids_d, mask_d)\n",
    "\n",
    "            # Soft Voting (DeBERTa 가중치 0.6)\n",
    "            probs = (torch.sigmoid(logits_bert) * 0.4) + (torch.sigmoid(logits_deberta) * 0.6)\n",
    "            batch_probs = probs.cpu().numpy()\n",
    "            \n",
    "            # 후처리\n",
    "            for sample_probs in batch_probs:\n",
    "                top_indices = sample_probs.argsort()[-10:][::-1]\n",
    "                candidate_set = set()\n",
    "                score_map = {}\n",
    "\n",
    "                for cid in top_indices:\n",
    "                    candidate_set.add(cid)\n",
    "                    score_map[cid] = float(sample_probs[cid])\n",
    "                    curr, depth = cid, 0\n",
    "                    while curr in parents_map and depth < 5:\n",
    "                        for pid in parents_map[curr]:\n",
    "                            if pid < NUM_CLASSES:\n",
    "                                candidate_set.add(pid)\n",
    "                                if pid not in score_map:\n",
    "                                    score_map[pid] = float(sample_probs[pid])\n",
    "                            curr = pid\n",
    "                        depth += 1\n",
    "                \n",
    "                sorted_candidates = sorted(list(candidate_set), key=lambda x: score_map.get(x, 0), reverse=True)\n",
    "                final_labels = sorted_candidates[:3]\n",
    "                \n",
    "                if len(final_labels) < 2:\n",
    "                    rem = [x for x in sample_probs.argsort()[::-1] if x not in final_labels]\n",
    "                    for r in rem:\n",
    "                        final_labels.append(r)\n",
    "                        if len(final_labels) >= 2:\n",
    "                            break\n",
    "                \n",
    "                all_preds.append(sorted(final_labels))\n",
    "\n",
    "    print(f\"4. Saving to {OUTPUT_CSV}...\")\n",
    "    with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\", \"labels\"])\n",
    "        for pid, labels in zip(test_pids, all_preds):\n",
    "            writer.writerow([pid, \",\".join(map(str, labels))])\n",
    "    \n",
    "    print(\"✅ Ensemble Done!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_ensemble()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
