{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410a3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Starting Training with microsoft/deberta-v3-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1659 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 1. 환경 설정 ---\n",
    "def seed_everything(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(42)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# [핵심 변경] 모델을 DeBERTa v3로 변경\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\" \n",
    "BATCH_SIZE = 16 # OOM 나면 8로 줄이세요\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-5 \n",
    "MAX_LEN = 256\n",
    "NUM_CLASSES = 531\n",
    "HIDDEN_DIM = 768\n",
    "\n",
    "# 경로 (기존과 동일)\n",
    "BASE_DIR = \"../Amazon_products\"\n",
    "TRAIN_CORPUS_PATH = os.path.join(BASE_DIR, \"train/train_corpus.txt\")\n",
    "HIERARCHY_PATH = os.path.join(BASE_DIR, \"class_hierarchy.txt\")\n",
    "SILVER_LABELS_PATH = \"train_round_2.csv\" \n",
    "OUTPUT_MODEL_DIR = \"saved_model_deberta_gnn\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_MODEL_DIR):\n",
    "    os.makedirs(OUTPUT_MODEL_DIR)\n",
    "\n",
    "# --- 2. 인접 행렬 생성 (기존과 동일) ---\n",
    "def build_adjacency_matrix(hierarchy_path, num_classes):\n",
    "    adj = torch.eye(num_classes)\n",
    "    with open(hierarchy_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                p, c = int(parts[0]), int(parts[1])\n",
    "                if p < num_classes and c < num_classes:\n",
    "                    adj[p, c] = 1\n",
    "                    adj[c, p] = 1\n",
    "    rowsum = torch.sum(adj, dim=1)\n",
    "    d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "    norm_adj = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "    return norm_adj.to(DEVICE)\n",
    "\n",
    "# --- 3. GCN Layer (기존과 동일) ---\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "# --- 4. [수정됨] DeBERTa 호환 GNN 모델 ---\n",
    "class DebertaGCN(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, hidden_dim, adj_matrix):\n",
    "        super(DebertaGCN, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # 라벨 임베딩\n",
    "        self.label_embedding = nn.Parameter(torch.FloatTensor(num_classes, hidden_dim))\n",
    "        nn.init.xavier_uniform_(self.label_embedding)\n",
    "        \n",
    "        self.gcn = GraphConvolution(hidden_dim, hidden_dim)\n",
    "        self.adj_matrix = adj_matrix\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # DeBERTa 출력 처리\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # [중요] DeBERTa는 pooler_output이 없을 수 있으므로 CLS 토큰(0번 인덱스)을 직접 가져옴\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        doc_embedding = last_hidden_state[:, 0, :] # [Batch, Hidden]\n",
    "        doc_embedding = self.dropout(doc_embedding)\n",
    "        \n",
    "        # GCN 처리\n",
    "        refined_label_embedding = self.gcn(self.label_embedding, self.adj_matrix)\n",
    "        refined_label_embedding = torch.tanh(refined_label_embedding)\n",
    "        \n",
    "        # 예측\n",
    "        logits = torch.mm(doc_embedding, refined_label_embedding.t())\n",
    "        return logits\n",
    "\n",
    "# --- 5. 학습 준비 ---\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label_list = [int(x) for x in str(self.labels[item]).split(\",\") if x]\n",
    "        label_tensor = torch.zeros(NUM_CLASSES)\n",
    "        label_tensor[label_list] = 1.0\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': label_tensor\n",
    "        }\n",
    "\n",
    "print(\"Loading data...\")\n",
    "pid2text = {}\n",
    "with open(TRAIN_CORPUS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        p, t = line.strip().split(\"\\t\", 1)\n",
    "        pid2text[int(p)] = t\n",
    "\n",
    "df = pd.read_csv(SILVER_LABELS_PATH)\n",
    "df['text'] = df['pid'].map(pid2text)\n",
    "df = df.dropna()\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "train_dataset = ReviewDataset(train_df.text.to_numpy(), train_df.labels.to_numpy(), tokenizer, MAX_LEN)\n",
    "val_dataset = ReviewDataset(val_df.text.to_numpy(), val_df.labels.to_numpy(), tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# --- 6. 학습 실행 ---\n",
    "adj_matrix = build_adjacency_matrix(HIERARCHY_PATH, NUM_CLASSES)\n",
    "model = DebertaGCN(MODEL_NAME, NUM_CLASSES, HIDDEN_DIM, adj_matrix).to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 0, total_steps)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"Starting Training with {MODEL_NAME}...\")\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        ids = batch['input_ids'].to(DEVICE)\n",
    "        mask = batch['attention_mask'].to(DEVICE)\n",
    "        lbls = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(ids, mask)\n",
    "        loss = loss_fn(logits, lbls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Train Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            ids = batch['input_ids'].to(DEVICE)\n",
    "            mask = batch['attention_mask'].to(DEVICE)\n",
    "            lbls = batch['labels'].to(DEVICE)\n",
    "            logits = model(ids, mask)\n",
    "            val_loss += loss_fn(logits, lbls).item()\n",
    "            \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        # 모델 저장\n",
    "        torch.save(model, os.path.join(OUTPUT_MODEL_DIR, \"best_model_deberta.pt\"))\n",
    "        print(\"Model Saved!\")\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
