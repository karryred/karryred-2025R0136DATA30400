{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1bbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python314\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Model & Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Scanning Test Data for Confident Predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/615 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# --- 1. 설정 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 531\n",
    "\n",
    "# 경로 설정\n",
    "BASE_DIR = \"../Amazon_products\"\n",
    "TEST_CORPUS_PATH = os.path.join(BASE_DIR, \"test/test_corpus.txt\")\n",
    "HIERARCHY_PATH = os.path.join(BASE_DIR, \"class_hierarchy.txt\")\n",
    "\n",
    "# 입력: 1대 모델 & 기존 학습 데이터\n",
    "INPUT_MODEL_PATH = \"saved_model/best_model.pt\"\n",
    "ORIGINAL_TRAIN_CSV = \"silver_labels_train.csv\"\n",
    "\n",
    "# 출력: 2라운드용 학습 데이터\n",
    "OUTPUT_TRAIN_CSV = \"train_round_2.csv\"\n",
    "\n",
    "# Self-Training 하이퍼파라미터\n",
    "CONFIDENCE_THRESHOLD = 0.85  \n",
    "\n",
    "# --- 2. 데이터 로더 ---\n",
    "def load_test_corpus(path):\n",
    "    pids, texts = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pids.append(parts[0])\n",
    "                texts.append(parts[1])\n",
    "    return pids, texts\n",
    "\n",
    "def load_hierarchy(path):\n",
    "    parents = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                parents[int(parts[1])] = int(parts[0]) # 자식: 부모\n",
    "    return parents\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "# --- 3. 메인 로직 ---\n",
    "def generate_pseudo_labels():\n",
    "    print(\"1. Loading Model & Data...\")\n",
    "    test_pids, test_texts = load_test_corpus(TEST_CORPUS_PATH)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_CLASSES)\n",
    "    model.load_state_dict(torch.load(INPUT_MODEL_PATH, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    dataset = InferenceDataset(test_texts, tokenizer)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    pseudo_labels = []\n",
    "    \n",
    "    print(\"2. Scanning Test Data for Confident Predictions...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(loader)):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probs = torch.sigmoid(outputs.logits)\n",
    "            \n",
    "            # 배치 내 샘플 반복\n",
    "            batch_probs = probs.cpu().numpy()\n",
    "            current_pids = test_pids[batch_idx * BATCH_SIZE : (batch_idx + 1) * BATCH_SIZE]\n",
    "            \n",
    "            for i, sample_prob in enumerate(batch_probs):\n",
    "                # 가장 높은 확률값들을 확인\n",
    "                top_indices = sample_prob.argsort()[::-1]\n",
    "                top_score = sample_prob[top_indices[0]] # 1등의 확률\n",
    "                \n",
    "                if top_score >= CONFIDENCE_THRESHOLD:\n",
    "                    # 2~3개 라벨 선정 (기존 추론 로직과 유사하게)\n",
    "                    # 여기서는 간단히 확률 높은 순으로 0.5 이상인 것들 (최대 3개)\n",
    "                    selected_indices = [idx for idx in top_indices if sample_prob[idx] > 0.5][:3]\n",
    "                    \n",
    "                    # 만약 0.5 넘는게 2개 미만이면, 상위 2개 강제 선택\n",
    "                    if len(selected_indices) < 2:\n",
    "                        selected_indices = top_indices[:2]\n",
    "                    \n",
    "                    # 계층 구조는 학습 시 모델이 배우게 놔두거나, 여기서 추가해줘도 됨.\n",
    "                    # 일단 모델이 예측한 그대로(selected_indices) 신뢰하고 추가.\n",
    "                    \n",
    "                    pid = current_pids[i]\n",
    "                    label_str = \",\".join(map(str, sorted(selected_indices)))\n",
    "                    pseudo_labels.append([pid, label_str])\n",
    "\n",
    "    print(f\"   -> Found {len(pseudo_labels)} high-confidence samples out of {len(test_texts)}.\")\n",
    "\n",
    "    # --- 4. 병합 및 저장 ---\n",
    "    print(\"3. Merging with Original Train Data...\")\n",
    "    \n",
    "    # 기존 Train 데이터 로드\n",
    "    df_train = pd.read_csv(ORIGINAL_TRAIN_CSV)\n",
    "    print(f\"   Original Train Size: {len(df_train)}\")\n",
    "    \n",
    "    # Pseudo Label 데이터프레임 생성\n",
    "    df_pseudo = pd.DataFrame(pseudo_labels, columns=['pid', 'labels'])\n",
    "    print(f\"   Pseudo Label Size: {len(df_pseudo)}\")\n",
    "    \n",
    "    # 병합 (concat)\n",
    "    df_round2 = pd.concat([df_train, df_pseudo], ignore_index=True)\n",
    "    \n",
    "    # 저장\n",
    "    df_round2.to_csv(OUTPUT_TRAIN_CSV, index=False)\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"✅ Round 2 Training Data Saved: {OUTPUT_TRAIN_CSV}\")\n",
    "    print(f\"   Total Training Samples: {len(df_round2)} (Increased by {len(df_pseudo)})\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Now, run 'train.py' again with this new CSV file!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_pseudo_labels()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
