{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f701e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 1. 설정 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\" \n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 531\n",
    "\n",
    "INPUT_MODEL_PATH = \"saved_model_deberta_gnn/best_model_deberta.pt\" \n",
    "OUTPUT_CSV = \"submission.csv\" \n",
    "\n",
    "# 경로\n",
    "BASE_DIR = \"Amazon_products\"\n",
    "TEST_CORPUS_PATH = os.path.join(BASE_DIR, \"test/test_corpus.txt\")\n",
    "HIERARCHY_PATH = os.path.join(BASE_DIR, \"class_hierarchy.txt\")\n",
    "\n",
    "# --- 2. 모델 클래스 정의 (학습 코드와 동일해야 함) ---\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "class DebertaGCN(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, hidden_dim, adj_matrix):\n",
    "        super(DebertaGCN, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.label_embedding = nn.Parameter(torch.FloatTensor(num_classes, hidden_dim))\n",
    "        self.gcn = GraphConvolution(hidden_dim, hidden_dim)\n",
    "        self.adj_matrix = adj_matrix\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        doc_embedding = last_hidden_state[:, 0, :] # CLS Token\n",
    "        doc_embedding = self.dropout(doc_embedding)\n",
    "        \n",
    "        refined_label_embedding = self.gcn(self.label_embedding, self.adj_matrix)\n",
    "        refined_label_embedding = torch.tanh(refined_label_embedding)\n",
    "        \n",
    "        logits = torch.mm(doc_embedding, refined_label_embedding.t())\n",
    "        return logits\n",
    "\n",
    "# --- 3. 데이터 로더 ---\n",
    "def load_test_corpus(path):\n",
    "    pids, texts = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pids.append(parts[0])\n",
    "                texts.append(parts[1])\n",
    "    return pids, texts\n",
    "\n",
    "def load_hierarchy(path):\n",
    "    parents = defaultdict(list)\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    p, c = int(parts[0]), int(parts[1])\n",
    "                    if p < NUM_CLASSES and c < NUM_CLASSES:\n",
    "                        parents[c].append(p)\n",
    "    return parents\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "# --- 4. 실행 로직 ---\n",
    "def run_inference():\n",
    "    print(\"1. Loading Data...\")\n",
    "    test_pids, test_texts = load_test_corpus(TEST_CORPUS_PATH)\n",
    "    parents_map = load_hierarchy(HIERARCHY_PATH)\n",
    "    \n",
    "    # [핵심 수정] use_fast=False 추가하여 에러 방지\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "    \n",
    "    dataset = TestDataset(test_texts, tokenizer, MAX_LEN)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    print(\"2. Loading Trained GNN Model...\")\n",
    "    if not os.path.exists(INPUT_MODEL_PATH):\n",
    "        print(f\"❌ Error: Model not found at {INPUT_MODEL_PATH}\")\n",
    "        return\n",
    "\n",
    "    # [핵심 수정] weights_only=False 추가하여 보안 에러 방지\n",
    "    model = torch.load(INPUT_MODEL_PATH, map_location=DEVICE, weights_only=False)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    all_final_labels = []\n",
    "    print(f\"3. Predicting on {len(test_texts)} samples...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"DeBERTa-GNN Inference\"):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            \n",
    "            # 후처리 (2~3개 라벨 선택)\n",
    "            for i in range(len(probs)):\n",
    "                sample_probs = probs[i]\n",
    "                top_indices = sample_probs.argsort()[-10:][::-1]\n",
    "                candidate_set = set()\n",
    "                score_map = {}\n",
    "\n",
    "                for cid in top_indices:\n",
    "                    candidate_set.add(cid)\n",
    "                    score_map[cid] = float(sample_probs[cid])\n",
    "                    curr = cid\n",
    "                    depth = 0\n",
    "                    while curr in parents_map and depth < 5:\n",
    "                        for pid in parents_map[curr]:\n",
    "                            if pid < NUM_CLASSES:\n",
    "                                candidate_set.add(pid)\n",
    "                                if pid not in score_map:\n",
    "                                    score_map[pid] = float(sample_probs[pid])\n",
    "                            curr = pid\n",
    "                        depth += 1\n",
    "                \n",
    "                sorted_candidates = sorted(list(candidate_set), key=lambda x: score_map.get(x, 0), reverse=True)\n",
    "                final_labels = sorted_candidates[:3]\n",
    "                \n",
    "                if len(final_labels) < 2:\n",
    "                    all_sorted = sample_probs.argsort()[::-1]\n",
    "                    for idx in all_sorted:\n",
    "                        if idx not in final_labels:\n",
    "                            final_labels.append(idx)\n",
    "                        if len(final_labels) >= 2:\n",
    "                            break\n",
    "                \n",
    "                all_final_labels.append(sorted(final_labels))\n",
    "\n",
    "    print(f\"4. Saving to {OUTPUT_CSV}...\")\n",
    "    with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\", \"labels\"])\n",
    "        for pid, labels in zip(test_pids, all_final_labels):\n",
    "            label_str = \",\".join(map(str, labels))\n",
    "            writer.writerow([pid, label_str])\n",
    "            \n",
    "    print(\"✅ Done! Ready to submit.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_inference()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
